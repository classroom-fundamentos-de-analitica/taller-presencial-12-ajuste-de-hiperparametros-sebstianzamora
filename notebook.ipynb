{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## APROXIMACION INGENUA\n",
    "##\n",
    "\n",
    "#\n",
    "# Empaquetado del entrenamiento del modelo\n",
    "#\n",
    "def train_estimator(alpha=0.5, l1_ratio=0.5, verbose=1):\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    import pandas as pd\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "    y = df[\"quality\"]\n",
    "    x = df.copy()\n",
    "    x.pop(\"quality\")\n",
    "\n",
    "    (x_train, x_test, y_train, y_test) = train_test_split(\n",
    "        x,\n",
    "        y,\n",
    "        test_size=0.25,\n",
    "        random_state=0,\n",
    "    )\n",
    "\n",
    "    estimator = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=12345)\n",
    "\n",
    "    estimator.fit(x_train, y_train)\n",
    "    y_pred = estimator.predict(x_test)\n",
    "\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    if verbose > 0:\n",
    "        print(estimator, \":\", sep=\"\")\n",
    "        print(f\"  MSE: {mse}\")\n",
    "        print(f\"  MAE: {mae}\")\n",
    "        print(f\"  R2: {r2}\")\n",
    "\n",
    "    if not os.path.exists(\"estimator.pickle\"):\n",
    "        saved_estimator = None\n",
    "    else:\n",
    "        with open(\"estimator.pickle\", \"rb\") as file:\n",
    "            saved_estimator = pickle.load(file)\n",
    "\n",
    "    if saved_estimator is None or estimator.score(\n",
    "        x_test, y_test\n",
    "    ) > saved_estimator.score(x_test, y_test):\n",
    "        with open(\"estimator.pickle\", \"wb\") as file:\n",
    "            pickle.dump(estimator, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_estimator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 13\u001b[0m, in \u001b[0;36mtrain_estimator\u001b[1;34m(alpha, l1_ratio, verbose)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElasticNet\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_absolute_error, mean_squared_error, r2_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "train_estimator(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.2, 0.2)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # ______________ REFACTORIZACION DE CODIGO ___________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Carga de datos\n",
    "#\n",
    "def load_data():\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n",
    "    df = pd.read_csv(url, sep=\";\")\n",
    "\n",
    "    y = df[\"quality\"]\n",
    "    x = df.copy()\n",
    "    x.pop(\"quality\")\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Particionamiento de datos\n",
    "#\n",
    "def make_train_test_split(x, y):\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    (x_train, x_test, y_train, y_test) = train_test_split(\n",
    "        x,\n",
    "        y,\n",
    "        test_size=0.25,\n",
    "        random_state=0,\n",
    "    )\n",
    "    return x_train, x_test, y_train, y_test\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Cálculo de metricas de evaluación\n",
    "#\n",
    "def eval_metrics(y_true, y_pred):\n",
    "\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    return mse, mae, r2\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Reporte de métricas de evaluación\n",
    "#\n",
    "def report(estimator, mse, mae, r2):\n",
    "\n",
    "    print(estimator, \":\", sep=\"\")\n",
    "    print(f\"  MSE: {mse}\")\n",
    "    print(f\"  MAE: {mae}\")\n",
    "    print(f\"  R2: {r2}\")\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Almacenamiento del modelo\n",
    "#\n",
    "def save_best_estimator(estimator):\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    with open(\"estimator.pickle\", \"wb\") as file:\n",
    "        pickle.dump(estimator, file)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Carga del modelo\n",
    "#\n",
    "def load_best_estimator():\n",
    "\n",
    "    import os\n",
    "    import pickle\n",
    "\n",
    "    if not os.path.exists(\"estimator.pickle\"):\n",
    "        return None\n",
    "    with open(\"estimator.pickle\", \"rb\") as file:\n",
    "        estimator = pickle.load(file)\n",
    "\n",
    "    return estimator\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "#\n",
    "# Entrenamiento\n",
    "#\n",
    "def train_estimator(alpha=0.5, l1_ratio=0.5, verbose=1):\n",
    "\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "    estimator = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=12345)\n",
    "    estimator.fit(x_train, y_train)\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    if verbose > 0:\n",
    "        report(estimator, mse, mae, r2)\n",
    "\n",
    "    best_estimator = load_best_estimator()\n",
    "    if best_estimator is None or estimator.score(x_test, y_test) > best_estimator.score(\n",
    "        x_test, y_test\n",
    "    ):\n",
    "        best_estimator = estimator\n",
    "\n",
    "    save_best_estimator(best_estimator)\n",
    "    \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.5, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.2, 0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(0.1, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_estimator():\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "    estimator = load_best_estimator()\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    report(estimator, mse, mae, r2)\n",
    "\n",
    "\n",
    "#\n",
    "# Debe coincidir con el mejor modelo encontrado en la celdas anteriores\n",
    "#\n",
    "check_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ___________ BUSQUEDA POR CODIGO _______________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hyperparameters_search(alphas, l1_ratios):\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for l1_ratio in l1_ratios:\n",
    "            train_estimator(alpha=alpha, l1_ratio=l1_ratio, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "alphas = np.linspace(0.0001, 0.5, 10)\n",
    "l1_ratios = np.linspace(0.0001, 0.5, 10)\n",
    "make_hyperparameters_search(alphas, l1_ratios)\n",
    "check_estimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "### __________________- USANDO GRIDSEARCH ____________________\n",
    "\n",
    "def train_estimator(alphas, l1_ratios, n_splits=5, verbose=1):\n",
    "\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "\n",
    "    # -------------------------------------------------------------------------\n",
    "    # Búsqueda de parámetros con validación cruzada\n",
    "    #\n",
    "    estimator = GridSearchCV(\n",
    "        estimator=ElasticNet(\n",
    "            random_state=12345,\n",
    "        ),\n",
    "        param_grid={\n",
    "            \"alpha\": alphas,\n",
    "            \"l1_ratio\": l1_ratios,\n",
    "        },\n",
    "        cv=n_splits,\n",
    "        refit=True,\n",
    "        verbose=0,\n",
    "        return_train_score=False,\n",
    "    )\n",
    "    # -------------------------------------------------------------------------\n",
    "\n",
    "    estimator.fit(x_train, y_train)\n",
    "\n",
    "    estimator = estimator.best_estimator_\n",
    "\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    if verbose > 0:\n",
    "        report(estimator, mse, mae, r2)\n",
    "\n",
    "    best_estimator = load_best_estimator()\n",
    "    if best_estimator is None or estimator.score(x_test, y_test) > best_estimator.score(\n",
    "        x_test, y_test\n",
    "    ):\n",
    "        best_estimator = estimator\n",
    "\n",
    "    save_best_estimator(best_estimator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "    \n",
    "import numpy as np\n",
    "\n",
    "train_estimator(\n",
    "    alphas=np.linspace(0.0001, 0.5, 10),\n",
    "    l1_ratios=np.linspace(0.0001, 0.5, 10),\n",
    "    n_splits=5,\n",
    "    verbose=1,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_estimator():\n",
    "\n",
    "    x, y = load_data()\n",
    "    x_train, x_test, y_train, y_test = make_train_test_split(x, y)\n",
    "    estimator = load_best_estimator()\n",
    "    mse, mae, r2 = eval_metrics(y_test, y_pred=estimator.predict(x_test))\n",
    "    report(estimator, mse, mae, r2)\n",
    "\n",
    "\n",
    "#\n",
    "# Debe coincidir con el mejor modelo encontrado en la celdas anteriores\n",
    "#\n",
    "check_estimator()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
